{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    norm = log(n * (2*np.pi)**0.5) / 2\n",
    "    f = lambda x, a, b: a + b * x\n",
    "    if s_int:\n",
    "        w = lambda b, s, dx, dy: ((b*dx)**2 + dy**2 + s**2)**0.5\n",
    "        def _loglike(p, x, y, *args):\n",
    "            wi = w(p[1], p[2], *args)\n",
    "            return norm + (2*log(wi) + ((y - f(x, *p[:2])) / wi)**2).sum()\n",
    "    else:\n",
    "        w = lambda b, dx, dy: ((b*dx)**2 + dy**2)**0.5\n",
    "        def _loglike(p, x, y, *args):\n",
    "            wi = w(p[1], *args)\n",
    "            return norm + (2*log(wi) + ((y - f(x, *p[:2])) / wi)**2).sum()\n",
    "        start = start[:2]\n",
    "\n",
    "    fit = fmin(_loglike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    # bootstrap errors?\n",
    "    if bootstrap is False:\n",
    "        return fit\n",
    "    jboot = np.random.randint(0, n, (bootstrap,n))\n",
    "    boot = [fmin(_loglike, start, args=(x1[j],x2[j],x1err[j],x2err[j]),\n",
    "                 disp=False, full_output=False)\n",
    "            for j in jboot]\n",
    "    out_err = np.std(boot, axis=0)\n",
    "    out = np.transpose([fit, out_err])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import optimize, stats\n",
    "import warnings\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    from itertools import izip\n",
    "else:\n",
    "    izip = zip\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_log(x, xerr=[], base=10, which='average'):\n",
    "    \"\"\"\n",
    "    Take linear measurements and uncertainties and transform to log\n",
    "    values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array of floats\n",
    "        measurements of which to take logarithms\n",
    "    Optional Parameters\n",
    "    -------------------\n",
    "    xerr : array of floats\n",
    "        uncertainties on x\n",
    "    base : float\n",
    "        base with which the logarithms should be calculated. FOR NOW USE\n",
    "        ONLY 10.\n",
    "    which : {'lower', 'upper', 'both', 'average'}\n",
    "        Which uncertainty to report; note that when converting to/from\n",
    "        linear and logarithmic spaces, errorbar symmetry is not\n",
    "        preserved. The following are the available options:\n",
    "            if which=='lower': logxerr = logx - log(x-xerr)\n",
    "            if which=='upper': logxerr = log(x+xerr) - logx\n",
    "        If `which=='both'` then both values are returned, and if\n",
    "        `which=='average'`, then the average of the two is returned.\n",
    "        Default is 'average'.\n",
    "    Returns\n",
    "    -------\n",
    "    logx : array of floats\n",
    "        values in log space, i.e., base**logx\n",
    "    logxerr : array of floats\n",
    "        log-uncertainties, as discussed above\n",
    "    \"\"\"\n",
    "    if np.iterable(x):\n",
    "        return_scalar = False\n",
    "    else:\n",
    "        return_scalar = True\n",
    "        x = [x]\n",
    "    x = np.array(x)\n",
    "    if not np.iterable(xerr):\n",
    "        xerr = [xerr]\n",
    "    if len(xerr) == 0:\n",
    "        xerr = np.zeros(x.shape)\n",
    "    else:\n",
    "        xerr = np.array(xerr)\n",
    "    assert xerr.shape == x.shape, \\\n",
    "        'The shape of x and xerr must be the same'\n",
    "    assert which in ('lower', 'upper', 'both', 'average'), \\\n",
    "        \"Valid values for optional argument `which` are 'lower', 'upper',\" \\\n",
    "        \" 'average' or 'both'.\"\n",
    "    logx = np.log10(x)\n",
    "    logxlo = logx - np.log10(x-xerr)\n",
    "    logxhi = np.log10(x+xerr) - logx\n",
    "    if return_scalar:\n",
    "        logx = logx[0]\n",
    "        logxlo = logxlo[0]\n",
    "        logxhi = logxhi[0]\n",
    "    if which == 'both':\n",
    "        return logx, logxlo, logxhi\n",
    "    if which == 'lower':\n",
    "        logxerr = logxlo\n",
    "    elif which == 'upper':\n",
    "        logxerr = logxhi\n",
    "    else:\n",
    "        logxerr = 0.5 * (logxlo+logxhi)\n",
    "    return logx, logxerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(x1, x2, x1err=[], x2err=[], start=(1.,1.,0.1), **kwargs):\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Estimation of best-fit parameters\n",
    "    Parameters\n",
    "    ----------\n",
    "      x1, x2    : float arrays\n",
    "                  the independent and dependent variables.\n",
    "      x1err, x2err : float arrays (optional)\n",
    "                  measurement uncertainties on independent and\n",
    "                  dependent variables. Any of the two, or both, can be\n",
    "                  supplied.\n",
    "      cerr      : float array (same size as x1)\n",
    "                  covariance on the measurement errors (NOT YET\n",
    "                  IMPLEMENTED)\n",
    "      s_int     : boolean (default True)\n",
    "                  whether to include intrinsic scatter in the MLE.\n",
    "      start     : tuple of floats\n",
    "                  initial guess for free parameters. If `s_int` is\n",
    "                  True, then po must have 3 elements; otherwise it can\n",
    "                  have two (for the zero point and the slope; any other\n",
    "                  elements will be discarded)\n",
    "      bootstrap : int or False\n",
    "                  if not False, it is the number of samples with which\n",
    "                  to estimate uncertainties on the best-fit parameters\n",
    "      logify    : boolean (default True)\n",
    "                  whether to convert the values to log10's. This is to\n",
    "                  calculate the best-fit power law. Note that the\n",
    "                  result is given for the equation log(y)=a+b*log(x) --\n",
    "                  i.e., the zero point must be converted to 10**a if\n",
    "                  `logify=True`\n",
    "      **kwargs  : dictionary (optional)\n",
    "                  arguments passed to `scipy.optimize.fmin`\n",
    "    Returns\n",
    "    -------\n",
    "      a         : float\n",
    "                  Maximum Likelihood Estimate of the zero point. Note\n",
    "                  that if logify=True, the power-law intercept is 10**a\n",
    "      b         : float\n",
    "                  Maximum Likelihood Estimate of the slope\n",
    "      s         : float (optional, if s_int=True)\n",
    "                  Maximum Likelihood Estimate of the intrinsic scatter\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    x1err = np.array(x1err)\n",
    "    x2err = np.array(x2err)  \n",
    "\n",
    "    log = np.log\n",
    "    fmin = optimize.fmin\n",
    "\n",
    "    norm = log(n * (2*np.pi)**0.5) / 2\n",
    "    f = lambda x, a, b: a + b * x\n",
    "    w = lambda b, s, dx, dy: ((b*dx)**2 + dy**2 + s**2)**0.5\n",
    "    def _loglike(p, x, y, *args):\n",
    "        wi = w(p[1], p[2], *args)\n",
    "        return norm + (2*log(wi) + ((y - f(x, *p[:2])) / wi)**2).sum()\n",
    "\n",
    "    fit = fmin(_loglike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mle_2(x1, x2, x1err=[], x2err=[], cerr=[], start=(1.,1.,0.1), **kwargs):\n",
    "    \"\"\"\n",
    "    mcmc version\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    x1err = np.array(x1err)\n",
    "    x2err = np.array(x2err)  \n",
    "    \n",
    "    log = np.log\n",
    "    pi = np.pi\n",
    "    fmin = optimize.fmin\n",
    "    def lnlike(theta, x, y, xerr, yerr):\n",
    "        \"\"\"Likelihood\"\"\"\n",
    "        a, b, s = theta\n",
    "        model = a + b*x\n",
    "        sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5\n",
    "        lglk = 2 * np.log(sigma).sum() + \\\n",
    "               (((y-model) / sigma)**2).sum() + \\\n",
    "               np.log(x.size) * (2*np.pi)**0.5 / 2\n",
    "        return lglk\n",
    "    fit = fmin(lnlike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit\n",
    " \n",
    "\n",
    "\n",
    "def mle_3(x1, x2, x1err=[], x2err=[], cerr=[], start=(1.,1.,0.1), **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    x1err = np.array(x1err)\n",
    "    x2err = np.array(x2err)  \n",
    "    \n",
    "    log = np.log\n",
    "    pi = np.pi\n",
    "    fmin = optimize.fmin\n",
    "    def lnlike(theta, x, y, xerr, yerr):\n",
    "        \"\"\"Likelihood\"\"\"\n",
    "        a, b, s = theta\n",
    "        model = a + b*x\n",
    "        sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5\n",
    "        lglk = 2 * np.log(sigma).sum() + \\\n",
    "               (((y-model) / sigma)**2).sum() + \\\n",
    "               n*np.log(2*np.pi)\n",
    "        return lglk\n",
    "    fit = fmin(lnlike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit\n",
    "\n",
    "\n",
    "\n",
    "def mle_4(x1, x2, x1err=[], x2err=[], cerr=[], start=(1.,1.,0.1), **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    x1err = np.array(x1err)\n",
    "    x2err = np.array(x2err)  \n",
    "    \n",
    "    log = np.log\n",
    "    pi = np.pi\n",
    "    fmin = optimize.fmin\n",
    "    def lnlike(theta, x, y, xerr, yerr):\n",
    "        \"\"\"Likelihood\"\"\"\n",
    "        a, b, s = theta\n",
    "        model = a + b*x\n",
    "        sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5\n",
    "        lglk = n*log(2*pi) + log(sigma**2).sum() + (((y-model)/sigma)**2).sum()\n",
    "        return lglk\n",
    "    fit = fmin(lnlike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit\n",
    "\n",
    "def mle_5(x1, x2, x1err=[], x2err=[], cerr=[], start=(1.,1.,0.1), **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    x1err = np.array(x1err)\n",
    "    x2err = np.array(x2err)  \n",
    "    \n",
    "    log = np.log\n",
    "    pi = np.pi\n",
    "    fmin = optimize.fmin\n",
    "    def lnlike(theta, x, y, xerr, yerr):\n",
    "        \"\"\"Likelihood\"\"\"\n",
    "        a, b, s = theta\n",
    "        model = a + b*x\n",
    "        sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5\n",
    "        lglk = 0.5*(n*log(2*pi) + log(sigma**2).sum() + (((y-model)/sigma)**2).sum())\n",
    "        return lglk\n",
    "    fit = fmin(lnlike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit\n",
    "\n",
    "\n",
    "def mle_6(x1, x2, x1err=[], x2err=[], cerr=[], start=(1.,1.,0.1), **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    x1err = np.array(x1err)\n",
    "    x2err = np.array(x2err)  \n",
    "    \n",
    "    log = np.log\n",
    "    pi = np.pi\n",
    "    fmin = optimize.fmin\n",
    "    def lnlike(theta, x, y, xerr, yerr):\n",
    "        \"\"\"Likelihood\"\"\"\n",
    "        a, b, s = theta\n",
    "        model = a + b*x\n",
    "        sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5\n",
    "        lglk = n*log(2*pi) + log(sigma).sum() + (((y-model)/sigma)**2).sum()\n",
    "        return lglk\n",
    "    fit = fmin(lnlike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    return fit\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(('/Users/kimzoldak/Github/GRB_Hubble_Diagram/data/'\n",
    "                    'Band_13_GBM+LAT__22_GBMconstrained.txt'), \n",
    "                   sep='\\t')\n",
    "\n",
    "data = data[data.trigger.map(lambda x: x != 'bn090510016')]  # remove short burst\n",
    "data.index = range(0, len(data))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['eiso'] = data['eiso']/1E52\n",
    "df['eiso_err'] = data.loc[:, ['eiso_err_low', 'eiso_err_up']].apply(np.mean, 1)\n",
    "df['eiso_err'] = df['eiso_err']/1E52\n",
    "df['epeakRest'] = data['epeakRest']\n",
    "df['epeakRest_err'] = data.loc[:, ['epeakRest_err_low', 'epeakRest_err_up']].apply(np.mean, 1)\n",
    "\n",
    "\n",
    "\n",
    "# pre-log data\n",
    "\n",
    "xdata,xdataerr = to_log(x=np.asarray(df['eiso']), \n",
    "                        xerr=np.asarray(df['eiso_err']))\n",
    "\n",
    "ydata,ydataerr = to_log(x=np.asarray(df['epeakRest']), \n",
    "                        xerr=np.asarray(df['epeakRest_err']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -62.342151\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93142871, 0.55633529, 0.23060952])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle( x1 = xdata, \n",
    "     x2 = ydata, \n",
    "     x1err = xdataerr,  \n",
    "     x2err= ydataerr, \n",
    "     start=(1.,1.,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -60.145163\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93142871, 0.55633529, 0.23060952])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_2(   x1 = xdata, \n",
    "         x2 = ydata, \n",
    "         x1err = xdataerr,  \n",
    "         x2err= ydataerr, \n",
    "         start=(1.,1.,1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to get the MCMC lnlike fn to match their MCMC fn, change the `yerr*2` in `sigma = ((b*xerr)**2 + yerr*2 + s**2)**0.5` to `yerr**2` so that it reads as `sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5`\n",
    "#### Also change the `return -lglk` to `return lglk`\n",
    "sigma = ((b*xerr)**2 + yerr**2 + s**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -2.076980\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93142871, 0.55633529, 0.23060952])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_3(   x1 = xdata, \n",
    "         x2 = ydata, \n",
    "         x1err = xdataerr,  \n",
    "         x2err= ydataerr, \n",
    "         start=(1.,1.,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -2.076980\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93142871, 0.55633529, 0.23060952])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_4(   x1 = xdata, \n",
    "         x2 = ydata, \n",
    "         x1err = xdataerr,  \n",
    "         x2err= ydataerr, \n",
    "         start=(1.,1.,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.038490\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93142871, 0.55633529, 0.23060952])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_5(   x1 = xdata, \n",
    "         x2 = ydata, \n",
    "         x1err = xdataerr,  \n",
    "         x2err= ydataerr, \n",
    "         start=(1.,1.,1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No matter what form of the equation you use (even if it has a minor error), you still get the same parameters, as long as sigma is correct. You will get different final values of the maximum likelihood (the negative number).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 42.005548\n",
      "         Iterations: 61\n",
      "         Function evaluations: 110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93172458, 0.55615883, 0.3291048 ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_6(   x1 = xdata, \n",
    "         x2 = ydata, \n",
    "         x1err = xdataerr,  \n",
    "         x2err= ydataerr, \n",
    "         start=(1.,1.,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
