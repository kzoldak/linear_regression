{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import optimize, stats\n",
    "import warnings\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    from itertools import izip\n",
    "else:\n",
    "    izip = zip\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(x1, x2, x1err=[], x2err=[], cerr=[], s_int=True, start=(1.,1.,0.1),\n",
    "        bootstrap=1000, logify=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Estimation of best-fit parameters\n",
    "    Parameters\n",
    "    ----------\n",
    "      x1, x2    : float arrays\n",
    "                  the independent and dependent variables.\n",
    "      x1err, x2err : float arrays (optional)\n",
    "                  measurement uncertainties on independent and\n",
    "                  dependent variables. Any of the two, or both, can be\n",
    "                  supplied.\n",
    "      cerr      : float array (same size as x1)\n",
    "                  covariance on the measurement errors (NOT YET\n",
    "                  IMPLEMENTED)\n",
    "      s_int     : boolean (default True)\n",
    "                  whether to include intrinsic scatter in the MLE.\n",
    "      start     : tuple of floats\n",
    "                  initial guess for free parameters. If `s_int` is\n",
    "                  True, then po must have 3 elements; otherwise it can\n",
    "                  have two (for the zero point and the slope; any other\n",
    "                  elements will be discarded)\n",
    "      bootstrap : int or False\n",
    "                  if not False, it is the number of samples with which\n",
    "                  to estimate uncertainties on the best-fit parameters\n",
    "      logify    : boolean (default True)\n",
    "                  whether to convert the values to log10's. This is to\n",
    "                  calculate the best-fit power law. Note that the\n",
    "                  result is given for the equation log(y)=a+b*log(x) --\n",
    "                  i.e., the zero point must be converted to 10**a if\n",
    "                  `logify=True`\n",
    "      **kwargs  : dictionary (optional)\n",
    "                  arguments passed to `scipy.optimize.fmin`\n",
    "    Returns\n",
    "    -------\n",
    "      a         : float\n",
    "                  Maximum Likelihood Estimate of the zero point. Note\n",
    "                  that if logify=True, the power-law intercept is 10**a\n",
    "      b         : float\n",
    "                  Maximum Likelihood Estimate of the slope\n",
    "      s         : float (optional, if s_int=True)\n",
    "                  Maximum Likelihood Estimate of the intrinsic scatter\n",
    "    \"\"\"\n",
    "    x1, x2 = np.array([x1, x2])\n",
    "    n = x1.size\n",
    "    if x2.size != n:\n",
    "        raise ValueError('x1 and x2 must have same length')\n",
    "    if len(x1err) == 0:\n",
    "        x1err = 1e-8 * np.absolute(x1.min()) * np.ones(n)\n",
    "    else:\n",
    "        x1err = np.array(x1err)\n",
    "    if len(x2err) == 0:\n",
    "        x2err = 1e-8 * np.absolute(x2.min()) * np.ones(n)\n",
    "    else:\n",
    "        x2err = np.array(x2err)  # kz\n",
    "        # x2err.array(x2err)  # theirs, bad\n",
    "    if logify:\n",
    "        x1, x1err = to_log(x1, x1err)\n",
    "        x2, x2err = to_log(x2, x2err)\n",
    "\n",
    "    log = np.log\n",
    "    fmin = optimize.fmin\n",
    "\n",
    "    norm = log(n * (2*np.pi)**0.5) / 2\n",
    "    f = lambda x, a, b: a + b * x\n",
    "    if s_int:\n",
    "        w = lambda b, s, dx, dy: ((b*dx)**2 + dy**2 + s**2)**0.5\n",
    "        def _loglike(p, x, y, *args):\n",
    "            wi = w(p[1], p[2], *args)\n",
    "            return norm + (2*log(wi) + ((y - f(x, *p[:2])) / wi)**2).sum()\n",
    "    else:\n",
    "        w = lambda b, dx, dy: ((b*dx)**2 + dy**2)**0.5\n",
    "        def _loglike(p, x, y, *args):\n",
    "            wi = w(p[1], *args)\n",
    "            return norm + (2*log(wi) + ((y - f(x, *p[:2])) / wi)**2).sum()\n",
    "        start = start[:2]\n",
    "\n",
    "    fit = fmin(_loglike, start, args=(x1,x2,x1err,x2err), **kwargs)\n",
    "    # bootstrap errors?\n",
    "    if bootstrap is False:\n",
    "        return fit\n",
    "    jboot = np.random.randint(0, n, (bootstrap,n))\n",
    "    boot = [fmin(_loglike, start, args=(x1[j],x2[j],x1err[j],x2err[j]),\n",
    "                 disp=False, full_output=False)\n",
    "            for j in jboot]\n",
    "    out_err = np.std(boot, axis=0)\n",
    "    out = np.transpose([fit, out_err])\n",
    "    return out\n",
    "\n",
    "\n",
    "def to_log(x, xerr=[], base=10, which='average'):\n",
    "    \"\"\"\n",
    "    Take linear measurements and uncertainties and transform to log\n",
    "    values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array of floats\n",
    "        measurements of which to take logarithms\n",
    "    Optional Parameters\n",
    "    -------------------\n",
    "    xerr : array of floats\n",
    "        uncertainties on x\n",
    "    base : float\n",
    "        base with which the logarithms should be calculated. FOR NOW USE\n",
    "        ONLY 10.\n",
    "    which : {'lower', 'upper', 'both', 'average'}\n",
    "        Which uncertainty to report; note that when converting to/from\n",
    "        linear and logarithmic spaces, errorbar symmetry is not\n",
    "        preserved. The following are the available options:\n",
    "            if which=='lower': logxerr = logx - log(x-xerr)\n",
    "            if which=='upper': logxerr = log(x+xerr) - logx\n",
    "        If `which=='both'` then both values are returned, and if\n",
    "        `which=='average'`, then the average of the two is returned.\n",
    "        Default is 'average'.\n",
    "    Returns\n",
    "    -------\n",
    "    logx : array of floats\n",
    "        values in log space, i.e., base**logx\n",
    "    logxerr : array of floats\n",
    "        log-uncertainties, as discussed above\n",
    "    \"\"\"\n",
    "    if np.iterable(x):\n",
    "        return_scalar = False\n",
    "    else:\n",
    "        return_scalar = True\n",
    "        x = [x]\n",
    "    x = np.array(x)\n",
    "    if not np.iterable(xerr):\n",
    "        xerr = [xerr]\n",
    "    if len(xerr) == 0:\n",
    "        xerr = np.zeros(x.shape)\n",
    "    else:\n",
    "        xerr = np.array(xerr)\n",
    "    assert xerr.shape == x.shape, \\\n",
    "        'The shape of x and xerr must be the same'\n",
    "    assert which in ('lower', 'upper', 'both', 'average'), \\\n",
    "        \"Valid values for optional argument `which` are 'lower', 'upper',\" \\\n",
    "        \" 'average' or 'both'.\"\n",
    "    logx = np.log10(x)\n",
    "    logxlo = logx - np.log10(x-xerr)\n",
    "    logxhi = np.log10(x+xerr) - logx\n",
    "    if return_scalar:\n",
    "        logx = logx[0]\n",
    "        logxlo = logxlo[0]\n",
    "        logxhi = logxhi[0]\n",
    "    if which == 'both':\n",
    "        return logx, logxlo, logxhi\n",
    "    if which == 'lower':\n",
    "        logxerr = logxlo\n",
    "    elif which == 'upper':\n",
    "        logxerr = logxhi\n",
    "    else:\n",
    "        logxerr = 0.5 * (logxlo+logxhi)\n",
    "    return logx, logxerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(('/Users/kimzoldak/Github/GRB_Hubble_Diagram/data/'\n",
    "                    'Band_13_GBM+LAT__22_GBMconstrained.txt'), \n",
    "                   sep='\\t')\n",
    "\n",
    "data = data[data.trigger.map(lambda x: x != 'bn090510016')]  # remove short burst\n",
    "data.index = range(0, len(data))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['eiso'] = data['eiso']/1E52\n",
    "df['eiso_err'] = data.loc[:, ['eiso_err_low', 'eiso_err_up']].apply(np.mean, 1)\n",
    "df['eiso_err'] = df['eiso_err']/1E52\n",
    "df['epeakRest'] = data['epeakRest']\n",
    "df['epeakRest_err'] = data.loc[:, ['epeakRest_err_low', 'epeakRest_err_up']].apply(np.mean, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eiso</th>\n",
       "      <th>eiso_err</th>\n",
       "      <th>epeakRest</th>\n",
       "      <th>epeakRest_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348.220960</td>\n",
       "      <td>7.055090</td>\n",
       "      <td>2731.929350</td>\n",
       "      <td>183.746653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410.198261</td>\n",
       "      <td>9.066845</td>\n",
       "      <td>3209.456160</td>\n",
       "      <td>243.199227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.813083</td>\n",
       "      <td>0.339776</td>\n",
       "      <td>1102.755808</td>\n",
       "      <td>89.911198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.085197</td>\n",
       "      <td>2.624849</td>\n",
       "      <td>2880.751218</td>\n",
       "      <td>51.922253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197.626135</td>\n",
       "      <td>1.059907</td>\n",
       "      <td>940.280908</td>\n",
       "      <td>12.310902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         eiso  eiso_err    epeakRest  epeakRest_err\n",
       "0  348.220960  7.055090  2731.929350     183.746653\n",
       "1  410.198261  9.066845  3209.456160     243.199227\n",
       "2   11.813083  0.339776  1102.755808      89.911198\n",
       "3  338.085197  2.624849  2880.751218      51.922253\n",
       "4  197.626135  1.059907   940.280908      12.310902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.54185491,  2.61299381,  1.07236324,  2.52902616,  2.29584438,\n",
       "         0.95559486,  0.377605  ,  1.95587216,  1.7817039 ,  2.22798906,\n",
       "         1.77951585,  1.27529528,  1.50577426,  1.5529516 ,  1.37900051,\n",
       "         0.6214454 ,  1.38880257,  0.59277242,  0.16047497,  0.80329796,\n",
       "         0.60744442,  1.10608739, -0.11498411,  1.41045805,  0.93947386,\n",
       "         0.39290272,  1.55206348,  0.55165741,  2.26252752,  1.28786324,\n",
       "         1.09571032,  0.45143842,  1.49451712,  0.98435176]),\n",
       " array([0.00880018, 0.00960102, 0.01249492, 0.00337187, 0.00232923,\n",
       "        0.01244287, 0.04578638, 0.01442056, 0.00722275, 0.00694363,\n",
       "        0.00707757, 0.00386576, 0.03001827, 0.00767281, 0.02603479,\n",
       "        0.01008966, 0.00669013, 0.02594262, 0.00749881, 0.03333501,\n",
       "        0.07933074, 0.05161403, 0.03345206, 0.03404931, 0.02511293,\n",
       "        0.03736718, 0.01840785, 0.02666182, 0.00572004, 0.04564094,\n",
       "        0.02216509, 0.01215324, 0.01984748, 0.01198458]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_log(x=np.asarray(df['eiso']), xerr=np.asarray(df['eiso_err']), base=10, which='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.43646946, 3.50643145, 3.04247935, 3.45950575, 2.97325762,\n",
       "        2.86648647, 2.33270509, 2.88375434, 2.99229292, 3.16820682,\n",
       "        3.05053867, 2.55857095, 2.85059   , 2.457404  , 2.79032391,\n",
       "        2.40343558, 2.43401211, 2.27210394, 1.74770618, 2.3255078 ,\n",
       "        2.53830136, 2.42366128, 2.38899218, 2.43898041, 2.11096192,\n",
       "        1.78440188, 2.74683131, 2.08541495, 3.46699154, 2.94254254,\n",
       "        2.32751543, 2.06024854, 2.66904495, 2.30113692]),\n",
       " array([0.02925435, 0.03297223, 0.03548819, 0.00782851, 0.00568645,\n",
       "        0.0262969 , 0.07435292, 0.0142109 , 0.02381436, 0.01517615,\n",
       "        0.02238772, 0.00669906, 0.06643355, 0.00690299, 0.029092  ,\n",
       "        0.01045163, 0.00907947, 0.01764416, 0.02456336, 0.04513165,\n",
       "        0.05735654, 0.05815116, 0.01947638, 0.08597159, 0.06594926,\n",
       "        0.04133206, 0.01782434, 0.07313081, 0.01196801, 0.08627908,\n",
       "        0.04017442, 0.01492702, 0.02392353, 0.01693227]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_log(x=np.asarray(df['epeakRest']), xerr=np.asarray(df['epeakRest_err']), base=10, which='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -62.342151\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.93142871, 0.1017148 ],\n",
       "       [0.55633529, 0.06159677],\n",
       "       [0.23060952, 0.02666611]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle( x1 = np.asarray(df['eiso']), \n",
    "     x2 = np.asarray(df['epeakRest']), \n",
    "     x1err = np.asarray(df['eiso_err']), \n",
    "     x2err= np.asarray(df['epeakRest_err']), \n",
    "     cerr=[],\n",
    "     s_int=True, \n",
    "     start=(1.,1.,1.),\n",
    "     bootstrap=1000, \n",
    "     logify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -62.342151\n",
      "         Iterations: 69\n",
      "         Function evaluations: 127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.93142871, 0.55633529, 0.23060952])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle( x1 = np.asarray(df['eiso']), \n",
    "     x2 = np.asarray(df['epeakRest']), \n",
    "     x1err = np.asarray(df['eiso_err']), \n",
    "     x2err= np.asarray(df['epeakRest_err']), \n",
    "     cerr=[],\n",
    "     s_int=True, \n",
    "     start=(1.,1.,1.),\n",
    "     bootstrap=False, \n",
    "     logify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5156.525643\n",
      "         Iterations: 33\n",
      "         Function evaluations: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.8054276 , 0.13768494],\n",
       "       [0.57929079, 0.09089536]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle( x1 = np.asarray(df['eiso']), \n",
    "     x2 = np.asarray(df['epeakRest']), \n",
    "     x1err = np.asarray(df['eiso_err']), \n",
    "     x2err= np.asarray(df['epeakRest_err']), \n",
    "     cerr=[],\n",
    "     s_int=False, \n",
    "     start=(1.,1.,1,),\n",
    "     bootstrap=1000, \n",
    "     logify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5156.525643\n",
      "         Iterations: 33\n",
      "         Function evaluations: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.8054276 , 0.57929079])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle( x1 = np.asarray(df['eiso']), \n",
    "     x2 = np.asarray(df['epeakRest']), \n",
    "     x1err = np.asarray(df['eiso_err']), \n",
    "     x2err= np.asarray(df['epeakRest_err']), \n",
    "     cerr=[],\n",
    "     s_int=False, \n",
    "     start=(1.,1.,1,),\n",
    "     bootstrap=False, \n",
    "     logify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are these results any different from what I get when I use the log-likelihood for a standard normal distribution without accounting for measurement errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.9977060123864732\n",
       " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ 0.00000000e+00,  0.00000000e+00, -1.77635684e-06])\n",
       "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     nfev: 96\n",
       "      nit: 14\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.55595838, 1.93207634, 0.23497343])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LogLikelihood(parameters, x, y):\n",
    "    m,b,sigma  = parameters\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    ymodel      = b + m*x\n",
    "    n           = float(len(ymodel))\n",
    "    err         = y-ymodel\n",
    "    f = ((n*np.log(2.*np.pi*sigma**2))/2) + (np.sum((err**2)/(2.*sigma**2)))\n",
    "    return f\n",
    "\n",
    "    #f = (-0.5*n*log(2.0*pi*sigma*sigma)) - (np.sum((err**2)/(2.0 * sigma*sigma)))\n",
    "    #f = (-0.5*n*log(2.*pi*sigma**2)) - (np.dot(error.T,error)/(2.*sigma**2))  \n",
    "    #return -1*f\n",
    "\n",
    "\n",
    "\n",
    "result = optimize.minimize(\n",
    "                            fun=LogLikelihood, \n",
    "                            x0=np.array([1,1,1]), \n",
    "                            method='L-BFGS-B', \n",
    "                            args= (np.asarray(df['eiso'].apply(np.log10)), \n",
    "                                   np.asarray(df['epeakRest'].apply(np.log10)))\n",
    "                          )\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `array([0.55595838, 1.93207634, 0.23497343])`  (standard normal dist with no errors)\n",
    "### verses \n",
    "### `array([1.93142871, 0.55633529, 0.23060952])` (accounting for errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is really no significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what is the point in accounting for errors if there is no difference between the function using errors and the funciton ignoring them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "## Lets re-write their MLE LogLikelihood to be set-up similar to ours:\n",
    "This is just to make sure we understand their setup and how it relates to ours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-log data\n",
    "xdata, xdataErr = to_log(x=np.asarray(df['eiso']), xerr=np.asarray(df['eiso_err']), which='average')\n",
    "ydata, ydataErr = to_log(x=np.asarray(df['epeakRest']), xerr=np.asarray(df['epeakRest_err']), which='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -62.34215104387983\n",
       " hess_inv: array([[ 3.44707508e-03, -2.05212931e-03,  5.08421099e-06],\n",
       "       [-2.05212931e-03,  1.59242249e-03, -1.14857093e-06],\n",
       "       [ 5.08421099e-06, -1.14857093e-06,  4.16291563e-04]])\n",
       "      jac: array([-2.86102295e-06, -2.86102295e-06,  9.53674316e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 105\n",
       "      nit: 15\n",
       "     njev: 21\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([1.93141678, 0.55633108, 0.23060075])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LogLikelihood_withErrors(parameters, x, y, xerr, yerr):\n",
    "    p = parameters\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    xerr = np.asarray(xerr)\n",
    "    yerr = np.asarray(yerr)\n",
    "    n = len(x)\n",
    "    \n",
    "    norm = np.log(n*(2*np.pi)**0.5)/2.\n",
    "    ymod = lambda x, a, b: a + b * x\n",
    "    sig = lambda b, s, dx, dy: ((b*dx)**2 + dy**2 + s**2)**0.5\n",
    "    return norm + (2*np.log(sig(p[1], p[2], xerr, yerr)) + ((y - ymod(x, *p[:2]))/sig(p[1], p[2], xerr, yerr) )**2).sum()\n",
    "\n",
    "\n",
    "\n",
    "result = optimize.minimize(\n",
    "                            fun=LogLikelihood_withErrors, \n",
    "                            x0=np.array([1.,1.,1.]), \n",
    "                            #method='L-BFGS-B', \n",
    "                            args= (xdata, ydata, xdataErr, ydataErr)\n",
    "                          )\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `array([1.93142871, 0.55633529, 0.23060952])`  (Theirs)\n",
    "### verses \n",
    "### `array([1.93141678, 0.55633108, 0.23060075])` (Ours)\n",
    "\n",
    "### There is no real difference, we successfully reproduced their function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you print (wi) in their function and print (print(sig(p[1], p[2], xerr, yerr))) in mine, you get idential values. Thus, the term accounting for uncertainties is the same in our setup as theirs.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
